import common
import torch
import os
from typing import List
from language_models import GPT, HuggingFace
from transformers import AutoModelForCausalLM, AutoTokenizer
from config import VICUNA_PATH, LLAMA_7B_PATH, LLAMA_13B_PATH, LLAMA_70B_PATH, LLAMA3_8B_PATH, LLAMA3_70B_PATH, GEMMA_2B_PATH, GEMMA_7B_PATH, MISTRAL_7B_PATH, MIXTRAL_7B_PATH, R2D2_PATH, PHI3_MINI_PATH, TARGET_TEMP, TARGET_TOP_P   


def load_target_model(target_model):
    targetLM = TargetLM(model_name = target_model, 
                        temperature = TARGET_TEMP, # init to 0
                        top_p = TARGET_TOP_P, # init to 1
                        )
    return targetLM

class TargetLM():
    """
    Base class for target language models.
    
    Generates responses for prompts using a language model. The self.model attribute contains the underlying generation model.
    """
    def __init__(self, 
            model_name: str, 
            temperature: float,
            top_p: float):
        
        self.model_name = model_name
        self.temperature = temperature
        self.top_p = top_p
        self.model, self.template = load_indiv_model(model_name)
        self.n_input_tokens = 0
        self.n_output_tokens = 0
        self.n_input_chars = 0
        self.n_output_chars = 0

    def get_response(self, prompts_list: List[str], max_n_tokens=None, temperature=None, no_template=False) -> List[dict]:
        batchsize = len(prompts_list)
        tokenizer = self.model.tokenizer
        convs_list = [common.conv_template(self.template) for _ in range(batchsize)]
        full_prompts = []  # batch of strings
        if no_template:
            full_prompts = prompts_list
        else:
            for conv, prompt in zip(convs_list, prompts_list):
                if "llama2" in self.model_name:
                    prompt = prompt + ' '
                conv.append_message(conv.roles[0], prompt)

                if "llama2" in self.model_name:
                    conv.append_message(conv.roles[1], None) 
                    formatted_prompt = '<s>' + conv.get_prompt()
                    full_prompts.append(formatted_prompt)
                else:
                    raise ValueError(f"To use {self.model_name}, first double check what is the right conversation template. This is to prevent any potential mistakes in the way templates are applied.")
        outputs = self.model.generate(full_prompts, 
                                      max_n_tokens=max_n_tokens,  
                                      temperature=self.temperature if temperature is None else temperature,
                                      top_p=self.top_p
        )
        
        self.n_input_tokens += sum(output['n_input_tokens'] for output in outputs)
        self.n_output_tokens += sum(output['n_output_tokens'] for output in outputs)
        self.n_input_chars += sum(len(full_prompt) for full_prompt in full_prompts)
        self.n_output_chars += len([len(output['text']) for output in outputs])
        return outputs


def load_indiv_model(model_name, device=None):
    model_path, template = get_model_path_and_template(model_name)
    model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.float16,
            # low_cpu_mem_usage=True, 
            device_map="auto",
            # token=os.getenv("HF_TOKEN"),
            # trust_remote_code=True).eval()
    )
    tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            use_fast=False,
        )

    tokenizer.pad_token = tokenizer.unk_token
    tokenizer.padding_side = 'left'
    
    lm = HuggingFace(model_name, model, tokenizer)
    
    return lm, template

def get_model_path_and_template(model_name):
    full_model_dict={
        "llama2":{
            "path":LLAMA_7B_PATH,
            "template":"llama-2"
        },
        "llama2-7b":{
            "path":LLAMA_7B_PATH,
            "template":"llama-2"
        },
        "llama2-13b":{
            "path":LLAMA_13B_PATH,
            "template":"llama-2"
        },
        "llama2-70b":{
            "path":LLAMA_70B_PATH,
            "template":"llama-2"
        },
        "llama3-8b":{
            "path":LLAMA3_8B_PATH,
            "template":"llama-2"
        },
        "llama3-70b":{
            "path":LLAMA3_70B_PATH,
            "template":"llama-2"
    }
    }
    # template = full_model_dict[model_name]["template"] if model_name in full_model_dict else "gpt-4"
    assert model_name in full_model_dict, f"Model {model_name} not found in `full_model_dict` (available keys {full_model_dict.keys()})"
    path, template = full_model_dict[model_name]["path"], full_model_dict[model_name]["template"]
    return path, template


    
